
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Why Language Models Hallucinate - Review</title>
  <style>
    body {
      background-color: #1a1a1a;
      color: #e0e0e0;
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      line-height: 1.7;
      padding: 40px;
    }
    h1 {
      font-size: 2.5em;
      text-align: center;
      background: linear-gradient(90deg, #ff6ec4, #7873f5);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      margin-bottom: 20px;
    }
    h2 {
      font-size: 1.8em;
      color: #ffb347;
      margin-top: 30px;
      margin-bottom: 10px;
      border-bottom: 2px solid #ffb347;
      display: inline-block;
      padding-bottom: 5px;
    }
    p {
      margin-bottom: 20px;
      color: #dcdcdc;
    }
    .highlight {
      color: #7bed9f;
      font-weight: bold;
    }
    .container {
      max-width: 900px;
      margin: auto;
      background: #2a2a2a;
      padding: 30px;
      border-radius: 15px;
      box-shadow: 0 0 20px rgba(255, 255, 255, 0.1);
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Why Language Models Hallucinate-OpenAI Research Paper Review By Masemene Matlakana Benny</h1>
    
    <p>The aim of the paper is to discuss why LLMs tend to suffer from hallucinations. What can be expected from the paper is focus on the cause of hallucinations in large language models. In the abstract ,it is highlighted that hallucinations are due to LLMs guessing even when uncertain of the answer hence they just take a shot at replying the user. As such the cause of hallucinations can be due uncertainty or being unfamiliar with certain words. Remember that several models like GPT are trained to generate text even as such they are expected to perform on unseen data just like in basic machine learning after we train the model.</p>

    <p>It should be noted that if their guessing over time improves then that results in improved product quality. However throughout the paper it is suggested that errors should expected from LLMs even after pre-training. But errors such as spelling mistakes are not classified as hallucinations. Moreover LLMs are said to be in test-mode always ,of which is true as they tend to be shipped AI-products which answers users in real-time hence they will always generate some text even when uncertain or never seen data at all. </p>

    <p>In my opinion I agree with the fact that LLMs tend to hallucinate even when uncertain. Imagine asking LLMs about a new completely language that cannot be found in any text or websie,you should expect hallucinations then .It is obvious that regardless of the choice of your LLM ,the LLM will not ignore your prompt and will obviously try at its best to return the best possible answer which is false and misleading information. The reason why I say that the LLM will not ignore your prompt is because it is trained to generate text so blanks or no answers are not to be returned by the LLM. No answer or blanks might have the user shifting to another available LLM.</p>
  </div>
</body>
</html>
